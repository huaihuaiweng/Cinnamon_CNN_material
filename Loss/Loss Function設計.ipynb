{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/MSE.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.constant([[1.0, 4.0, 2.0], [1.0, 2.0, 3.0], [2.0, 3.0, 1.0]])\n",
    "ground_truth = tf.constant([[1.0, 1.0, 1.0], [2.0, 100.0, 2.0], [1.0, 1.0, 1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一種方法算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8888888\n"
     ]
    }
   ],
   "source": [
    "#Square_Error = tf.square(ground_truth -prediction)\n",
    "Square_Error = tf.square(ground_truth -prediction)\n",
    "mse = tf.reduce_mean(Square_Error) \n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(Square_Error))\n",
    "    print(sess.run(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二種方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8888888\n"
     ]
    }
   ],
   "source": [
    "#  sum(error ** 2) / 2\n",
    "mse1 =(2.0*tf.nn.l2_loss(ground_truth - prediction))/tf.reshape(prediction,[-1]).get_shape().as_list()[0]\n",
    "with tf.Session() as sess:\n",
    "    #print(tf.reshape(prediction,[-1]).get_shape().as_list()[0])\n",
    "    print(sess.run(mse1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三種方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8888888\n"
     ]
    }
   ],
   "source": [
    "mse2 = tf.losses.mean_squared_error(ground_truth, prediction)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(mse2)) #1.8888888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/MAE.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "##按照公式\n",
    "residuals = tf.abs(tf.subtract(ground_truth ,prediction))\n",
    "mae=tf.reduce_mean(residuals)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(mae))\n",
    "    \n",
    "##套用API\n",
    "mae1 = tf.losses.absolute_difference(ground_truth, prediction)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/HuberLoss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888889\n",
      "0.8888889\n"
     ]
    }
   ],
   "source": [
    "## 參考：https://blog.csdn.net/hyk_1996/article/details/79570915\n",
    "## 按照原理\n",
    "def huber_loss(labels, predictions, delta=1.0):\n",
    "    error = tf.abs(tf.subtract(ground_truth ,prediction))\n",
    "    condition = tf.less(error, delta)\n",
    "    small_res = 0.5 * tf.square(error) ##誤差小遵照L2\n",
    "    large_res = delta * error - 0.5 * tf.square(delta) ## 誤差大採用線性誤差\n",
    "    return tf.where(condition, small_res, large_res)\n",
    "\n",
    "Huber=tf.reduce_mean(huber_loss(ground_truth, prediction, delta=2.0))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(Huber))\n",
    "\n",
    "## Call Tensorflow API\n",
    "Huber_loss=tf.losses.huber_loss(ground_truth,prediction,delta=2.0)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(Huber_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1=tf.reduce_mean(huber_loss(ground_truth, prediction, delta=1.0))\n",
    "loss_2=tf.reduce_mean(huber_loss(ground_truth, prediction, delta=2.0))\n",
    "loss_3=tf.reduce_mean(huber_loss(ground_truth, prediction, delta=3.0))\n",
    "loss_5=tf.reduce_mean(huber_loss(ground_truth, prediction, delta=5.0))\n",
    "loss_10=tf.reduce_mean(huber_loss(ground_truth, prediction, delta=10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5\n",
      "22.444445\n",
      "33.11111\n",
      "54.0\n",
      "104.27778\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(loss_1))\n",
    "    print(sess.run(loss_2))\n",
    "    print(sess.run(loss_3))\n",
    "    print(sess.run(loss_5))\n",
    "    print(sess.run(loss_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/CrossEntropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.constant([[1.0, 4.0, 2.0], [1.0, 2.0, 3.0], [2.0, 3.0, 1.0]])\n",
    "ground_truth = tf.constant([[0, 1, 0], [0, 0, 1], [1, 0, 0]],dtype=tf.float32) ## OneHotLabel\n",
    "ground_truth_1= tf.constant([1,2,0]) ## index label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66168594\n",
      "0.66168594\n",
      "0.66168594\n",
      "0.13781698\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##按照原理\n",
    "log_sm_vals = tf.nn.log_softmax(prediction)\n",
    "individual_loss = tf.reduce_sum(-1.0*tf.math.multiply(ground_truth, log_sm_vals), axis=1)\n",
    "loss = tf.reduce_mean(individual_loss)\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(individual_loss))\n",
    "    print(sess.run(loss)) ##0.66168594\n",
    "    \n",
    "## API-1\n",
    "cross_entropy_sum=tf.nn.softmax_cross_entropy_with_logits_v2(ground_truth,prediction)\n",
    "cross_entropy=tf.reduce_mean(cross_entropy_sum)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy)) ##0.66168594\n",
    "    \n",
    "## API-2 Sparse_Softmax，輸入不是OneHot\n",
    "sparse=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=ground_truth_1,logits=prediction))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(sparse)) ##0.66168594\n",
    "    \n",
    "    \n",
    "##與MSE的差異\n",
    "mse = tf.losses.mean_squared_error(ground_truth, tf.nn.softmax(prediction))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(mse)) ##0.13781698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98529524\n"
     ]
    }
   ],
   "source": [
    "## Binary Cross Entropy\n",
    "sigmoid=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=[0.,1.,0.],logits=[0.23,1.23,1.72]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(sigmoid)) ##0.98529524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE v.s CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/CrossEntropyMSE.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020202707317519466\n",
      "0.0004000000000000007\n"
     ]
    }
   ],
   "source": [
    "print(-1*np.log(prediction))\n",
    "print(np.mean([(1-prediction)**2,((1-prediction)-0)**2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/FocalLossFormula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss():\n",
    "    def __init__(self, gamma=2, alpha=[0.1,0.2,0.4], size_average=True):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, prediction, ground_truth):\n",
    "\n",
    "        softmax_prediction=tf.nn.softmax(prediction)\n",
    "        softmax_prediction=tf.reduce_sum(softmax_prediction*ground_truth,1)\n",
    "        logpt=tf.log(softmax_prediction)\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            at = tf.constant(self.alpha)\n",
    "\n",
    "        loss = -1*at * (1-softmax_prediction)**self.gamma * logpt\n",
    "        if self.size_average: return tf.reduce_mean(loss)\n",
    "        else: return tf.reduce_sum(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11024303\n",
      "0.661686\n",
      "0.66168594\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.constant([[1.0, 4.0, 2.0], [1.0, 2.0, 3.0], [2.0, 3.0, 1.0]],dtype=tf.float32)\n",
    "ground_truth = tf.constant([[0, 1, 0], [0, 0, 1], [1, 0, 0]],dtype=tf.float32) ## OneHotLabel\n",
    "\n",
    "## Alpha為頻率倒數，gamma先設為2\n",
    "with tf.Session() as sess:\n",
    "    Focal=FocalLoss( )\n",
    "    print(sess.run(Focal.forward(prediction,ground_truth))) #0.11024303\n",
    "    \n",
    "\n",
    "##當gamma=0,alpha=1時，就是CE\n",
    "with tf.Session() as sess:\n",
    "    Focal=FocalLoss(gamma=0,alpha=[1.,1.,1.])\n",
    "    print(sess.run(Focal.forward(prediction,ground_truth))) #0.661686\n",
    "    \n",
    "\n",
    "##會與上方得到數值相同\n",
    "cross_entropy_sum=tf.nn.softmax_cross_entropy_with_logits_v2(ground_truth,prediction)\n",
    "cross_entropy=tf.reduce_mean(cross_entropy_sum)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy)) #0.661686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/CenterLossformula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "##參考 https://github.com/EncodeTS/TensorFlow_Center_Loss\n",
    "def get_center_loss(features, labels, alpha=0.1, num_classes=3):\n",
    "\n",
    "    ##Embedding的維度\n",
    "    len_features = features.get_shape()[1]\n",
    "\n",
    "    ##初始化center\n",
    "    centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "    \n",
    "    centers_batch = tf.gather(centers, labels)\n",
    "    \n",
    "    ##計算Loss\n",
    "    loss = tf.nn.l2_loss(features - centers_batch)\n",
    "    \n",
    "    \n",
    "    #更新各個類別的Center\n",
    "    diff = centers_batch - features\n",
    "    \n",
    "    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
    "    appear_times = tf.gather(unique_count, unique_idx)\n",
    "    appear_times = tf.reshape(appear_times, [-1, 1])\n",
    "    \n",
    "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
    "    diff = alpha * diff\n",
    "    \n",
    "    centers_update_op = tf.scatter_sub(centers, labels, diff)\n",
    "    \n",
    "    return loss, centers, centers_update_op\n",
    "\n",
    "tf.reset_default_graph()\n",
    "Embedding=tf.Variable([[1,2,],[2,3],[1,2]],dtype=tf.float32)\n",
    "loss,_,_=get_center_loss(Embedding, [2,0,1], alpha=0.1, num_classes=3)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LossFunction/FocalLossFormula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9789365\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "##Label\n",
    "label=tf.constant([0,1,2,1,2,1,0,1],dtype=tf.int32)\n",
    "##Embedding output\n",
    "Embedding_out_out=tf.Variable(np.random.randn(8,256),dtype=tf.float32)\n",
    "##經過L2-normalization\n",
    "prediction_semi=tf.nn.l2_normalize(Embedding_out_out,axis=1)\n",
    "##計算 triplet loss\n",
    "loss_semi=tf.contrib.losses.metric_learning.triplet_semihard_loss(label,prediction_semi)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(loss_semi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand On"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model\n",
    "## 1. Huber Loss\n",
    "# Classification Model\n",
    "## 1.Triplet Loss+Softmax\n",
    "## 2.Center Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 補充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reorganize_data(x, y,class_=13925):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    dataset = {i: [] for i in range(class_)}\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        dataset[y[i][0]].append(x[i][0])\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def get_batch(dataset,k=4,class_=13925):\n",
    "    # Sample BATCH_K random images from each category of the MNIST dataset,\n",
    "    # returning the data along with its labels\n",
    "    batch = []\n",
    "    labels = []\n",
    "    \n",
    "    for l in range(class_):\n",
    "        indices = rm.sample(range(len(dataset[l])), k)\n",
    "        indices = np.array(indices)\n",
    "\n",
    "        batch.append([dataset[l][i] for i in indices])\n",
    "        labels += [l] * k\n",
    "\n",
    "    batch = np.array(batch).reshape(class_ * k)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    \n",
    "    return batch, labels\n",
    "\n",
    "def Train_Read(batch_,labels_,batch_batch,batch=100,k=4):\n",
    "    data_batch_x=[]\n",
    "    data_batch_y=[]\n",
    "    global random_choice\n",
    "    random_choice=rm.sample(batch_batch,int(batch/k))\n",
    "    for a in random_choice:\n",
    "        data_batch_x.append(batch_[a:a+k])\n",
    "        data_batch_y.append(labels_[a:a+k])\n",
    "    data_batch_x=np.array(data_batch_x).reshape(batch)\n",
    "    data_batch_y=np.array(data_batch_y).reshape(batch,1)\n",
    "    Batch_Read=[]\n",
    "    for path in data_batch_x:\n",
    "        img = image.load_img(path, target_size=(28, 28))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        x=x.reshape(28, 28, 3)\n",
    "        Batch_Read.append(x)\n",
    "    Batch_Read=np.array(Batch_Read)\n",
    "    return Batch_Read,data_batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label_X 是list train_data\n",
    "#Label_Y 是list train)_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_=Reorganize_data(Label_X,Label_Y)\n",
    "\n",
    "iteration = int((4*13925)/100)\n",
    "batch_batch=[x for x in range(0,len(dataset_),4)]\n",
    "batch_,labels_=get_batch(dataset_, 4,class_=13925)\n",
    "\n",
    "\n",
    "train_batch_x,train_batch_y=Train_Read(batch_,labels_,batch_batch,100,4)\n",
    "for _number_ in random_choice:\n",
    "    batch_batch.remove(_number_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
